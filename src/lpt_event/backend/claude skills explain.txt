âº Claude Code Skills Explained

  What Are Skills?

  Skills are specialized prompt templates that extend Claude Code with domain-specific capabilities. Think of them as expert personas that Claude can adopt to perform specific types of tasks more effectively.

  Skills are:
  - Context-aware: They include specialized knowledge and best practices for their domain
  - Template-driven: They provide structured approaches to common tasks
  - Auto-invoked: Claude automatically uses them based on your natural language requests
  - Customizable: You can create your own skills for your specific workflows

  Your Available Skill: data-documentation-generator

  You currently have one custom skill installed:

  ðŸ“š data-documentation-generator

  Purpose: Specialized assistant for documenting data engineering, analytics, and machine learning projects.

  Domain Expertise:
  - Python (standard libraries, data science, PySpark)
  - SQL (queries, CTEs, window functions, optimization)
  - Apache Spark (DataFrames, transformations, performance)
  - Jupyter/Databricks notebooks
  - Data pipelines and ETL workflows

  Documentation Standards:
  - Python: Google-style docstrings (also supports NumPy, Sphinx)
  - SQL: Structured header comments with purpose, inputs, outputs
  - Markdown: For README files and external documentation
  - Data dictionaries: Comprehensive schema documentation
  - Pipeline diagrams: Mermaid diagrams for visualization

  ---
  The 9 Commands Available

  1. generate-readme

  Creates a comprehensive README.md for data projects.

  When to use: Starting a new project or improving existing documentation.

  What it generates:
  - Project title and business purpose
  - Architecture overview with diagrams
  - Data sources and destinations
  - Prerequisites (Python, Spark versions, dependencies)
  - Installation and configuration instructions
  - Usage examples for pipelines and jobs
  - Testing, deployment, and monitoring guides

  Example request:
  "Generate a README for this data project"
  "Create project documentation"

  ---
  2. document-python-api

  Generates comprehensive Google-style docstrings for Python functions and classes.

  When to use: After writing Python code that needs documentation.

  What it adds:
  - Function/method docstrings with purpose and description
  - Parameter documentation with types and descriptions
  - Return value documentation
  - Exception documentation
  - Usage examples for complex functions
  - Performance notes for data-intensive operations
  - Type hints if missing

  Example request:
  "Document my Python functions"
  "Add docstrings to this code"

  Output example:
  def process_data(df: DataFrame, threshold: int = 100) -> DataFrame:
      """Process input DataFrame by filtering and aggregating.
      
      Args:
          df (DataFrame): Input Spark DataFrame with schema:
              - customer_id (string): Customer identifier
              - amount (double): Transaction amount
          threshold (int, optional): Minimum amount to include. Defaults to 100.
      
      Returns:
          DataFrame: Aggregated results with schema:
              - customer_id (string): Customer identifier
              - total_amount (double): Sum of amounts
      
      Note:
          This function performs a shuffle operation. Consider partitioning
          input data by customer_id for better performance.
      """

  ---
  3. document-sql-queries

  Adds comprehensive comments to SQL scripts and queries.

  When to use: Working with SQL files or embedded SQL in Python.

  What it adds:
  - Header comments explaining business logic
  - Input tables and output destinations
  - Key transformations documented
  - Performance notes (indexes, estimated rows, execution time)
  - Complex join explanations
  - CTE (Common Table Expression) documentation
  - Window function explanations
  - Data quality filter notes

  Example request:
  "Document this SQL query"
  "Add comments to my SQL scripts"

  Output example:
  /*
  Query Name: Customer Revenue Summary
  Purpose: Calculate total revenue per customer for reporting

  Input Tables:
    - transactions: Raw transaction data
    - customers: Customer master data

  Output: customer_revenue_summary table

  Key Transformations:
    1. Join transactions with customers
    2. Filter last 30 days
    3. Aggregate by customer with sum

  Performance Notes:
    - Uses index on transactions.customer_id
    - Estimated rows: 100K
    - Execution time: ~30s

  Author: Generated by Claude
  Date: 2024-01-09
  */

  ---
  4. document-spark-jobs

  Documents PySpark transformations and Spark SQL queries.

  When to use: Working with Spark DataFrames or Spark jobs.

  What it documents:
  - DataFrame transformation logic step-by-step
  - Partition strategies and their rationale
  - Broadcast joins and optimizations
  - DataFrame schemas
  - UDFs (User Defined Functions)
  - Caching and persistence strategies
  - Shuffle operations and performance implications
  - Spark configurations used
  - Examples with sample data

  Example request:
  "Document this Spark job"
  "Explain these PySpark transformations"

  ---
  5. generate-data-dictionary

  Creates a comprehensive DATA_DICTIONARY.md file.

  When to use: Documenting database schemas or DataFrame structures.

  What it generates:
  - Table/DataFrame descriptions with business context
  - Column-by-column documentation:
    - Data types and nullability
    - Business meaning
    - Example values
    - Validation rules
  - Primary and foreign keys
  - Partitioning/bucketing strategies
  - Data quality rules
  - Refresh frequency and retention policies
  - Sample queries

  Example request:
  "Create a data dictionary"
  "Document the database schema"

  Output format:
  ## Table: events

  **Description**: Event management data for tracking meetups and conferences

  **Refresh Frequency**: Real-time via API

  ### Schema

  | Column | Type | Nullable | Description | Example | Rules |
  |--------|------|----------|-------------|---------|-------|
  | id | INTEGER | NOT NULL | Primary key | 123 | Auto-increment |
  | title | VARCHAR(255) | NOT NULL | Event title | "Data Summit 2024" | Max 255 chars |
  | cost_usd | DECIMAL(10,2) | NOT NULL | Cost in USD | 99.99 | Must be >= 0 |

  ---
  6. document-pipeline

  Creates comprehensive pipeline documentation (PIPELINE.md).

  When to use: Documenting ETL workflows, data pipelines, or orchestration.

  What it generates:
  - End-to-end data flow diagrams (Mermaid)
  - Pipeline stages and dependencies
  - Orchestration details (Airflow, Luigi, etc.)
  - Scheduling and triggers
  - Data volume estimates
  - SLAs and performance targets
  - Error handling and retry logic
  - Monitoring and alerting setup
  - Rollback procedures

  Example request:
  "Document this data pipeline"
  "Create pipeline documentation"

  ---
  7. add-inline-comments

  Adds explanatory inline comments to complex code.

  When to use: Code is complex and needs clarification (this is what we used earlier!).

  What it adds:
  - Business logic explanations
  - Complex transformation rationale
  - Non-obvious SQL join explanations
  - Spark optimization choices
  - Data quality check purposes
  - Magic number explanations
  - Performance consideration notes
  - Workaround documentation

  Focus: WHY over WHAT (explains reasoning, not just what the code does)

  Example request:
  "Add inline comments to complex code"
  "Explain this code with comments"

  ---
  8. generate-notebook-docs

  Adds markdown documentation to Jupyter/Databricks notebooks.

  When to use: Working with notebooks that need better documentation.

  What it adds:
  - Markdown cell at top with notebook overview
  - Purpose and use case documentation
  - Section headers between code blocks
  - Input parameter and widget documentation
  - Code cell explanations
  - Visualization interpretations
  - Execution time estimates
  - Output artifact documentation
  - Troubleshooting tips
  - Links to related notebooks

  Example request:
  "Document this Jupyter notebook"
  "Add documentation to my notebook"

  ---
  9. audit-documentation

  Reviews existing documentation for completeness and consistency.

  When to use: Checking if your project documentation is complete.

  What it does:
  - Scans Python files for undocumented functions
  - Checks SQL scripts for missing comments
  - Verifies Spark jobs have documentation
  - Validates README completeness
  - Checks data dictionary coverage
  - Identifies outdated information
  - Verifies code examples still work
  - Generates gap report with suggestions
  - Checks terminology consistency

  Example request:
  "Audit my documentation"
  "Check documentation coverage"
  "Find documentation gaps"

  ---
  How to Use Skills

  Natural Language Invocation

  Simply ask Claude naturally - the skill is automatically selected:

  âœ… "Document my Python functions"
  âœ… "Create a README for this project"
  âœ… "Add inline comments to complex code"
  âœ… "Generate a data dictionary"

  What Claude Does Automatically

  1. Detects intent from your request
  2. Selects appropriate skill (data-documentation-generator)
  3. Chooses the right command (e.g., document-python-api)
  4. Applies domain knowledge from the skill
  5. Follows best practices defined in the skill
  6. Generates documentation matching the templates

  Skill Context

  The skill automatically considers these file types:
  - **/*.py - Python scripts
  - **/*.sql - SQL files
  - **/*.ipynb - Jupyter notebooks
  - **/*.pyspark - PySpark scripts
  - **/requirements.txt, **/pyproject.toml - Dependencies
  - **/*.yaml, **/*.json - Configuration files
  - **/README.md, **/DATA_DICTIONARY.md - Docs

  And ignores:
  - **/__pycache__/**, **/*.pyc
  - **/venv/**, **/.venv/**, **/node_modules/**
  - **/.git/**, **/logs/**
  - **/data/raw/**, **/data/processed/**

  ---
  Key Advantages

  1. Domain Expertise

  The skill knows data engineering best practices:
  - How to document Spark shuffle operations
  - SQL performance considerations
  - DataFrame schema documentation patterns
  - Data quality rule documentation

  2. Consistency

  All documentation follows the same patterns:
  - Google-style docstrings for Python
  - Structured SQL header comments
  - Standardized README sections
  - Uniform data dictionary format

  3. Completeness

  The skill ensures comprehensive coverage:
  - Function parameters, returns, exceptions
  - Performance notes for data operations
  - Business context in data dictionaries
  - End-to-end pipeline documentation

  4. Efficiency

  Saves time by:
  - Using templates for common patterns
  - Following industry standards automatically
  - Generating boilerplate documentation
  - Maintaining consistent style

  ---
  Example: How We Used It

  Earlier, you asked: "Document my Python functions"

  What happened:
  1. âœ… Skill detected: data-documentation-generator
  2. âœ… Command selected: document-python-api
  3. âœ… Scanned: All Python files in src/lpt_event/backend/
  4. âœ… Generated: Google-style docstrings for:
    - Classes (Runtime, DatabaseConfig, AppConfig, CustomFormatter)
    - Methods (ws, engine_url, get_session, validate_db, initialize_models)
    - Functions (add_not_found_handler, setup_logger, get_logger)
  5. âœ… Result: 6 files enhanced with comprehensive documentation

  ---
  Creating Your Own Skills

  You can create custom skills for your workflows:

  ~/.claude/skills/your-skill-name/
  â””â”€â”€ SKILL.md

  Skills can be:
  - Personal: ~/.claude/skills/ (just for you)
  - Project: .claude/skills/ (shared with team via git)
  - Enterprise: Managed centrally

  Check the Claude Code documentation for more details on creating custom skills!

  ---
  Summary

  Your data-documentation-generator skill is a specialized documentation assistant that:
  - âœ… Understands data engineering workflows
  - âœ… Follows industry documentation standards
  - âœ… Generates comprehensive, consistent documentation
  - âœ… Saves time on repetitive documentation tasks
  - âœ… Works automatically when you ask in natural language

  Just ask Claude naturally, and the right skill command will be invoked automatically! ðŸŽ¯